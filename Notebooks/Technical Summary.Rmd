---
title: 'Project Technical Discussion: A Data-based Approach to Prevent Lower Body Muscle Strains in Soccer'
author: "Tamas Koncz"
date: "August 21, 2018"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
bibliography: references.bib
---

```{r setup, include = FALSE}
library(knitr)
library(kableExtra)
library(pander)
library(tidyverse)
library(caret)
library(DMwR)
library(glmnet)
library(xgboost)
library(ranger)

opts_chunk$set(warning = FALSE)
opts_chunk$set(message = FALSE)
opts_chunk$set(echo    = FALSE)
```
  
# Introduction 

The aim of this work is to identify direct links between injury risk of professional soccer players and certain relatable factors.  

Specifically, the injuries in focus are muscle strains happening in the lower body. To consider the seriousness of this problem, consider the below chart:  
```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/xxx. Lost player days due to lower body strains.jpeg")
```

A large number of matches are missed by players due to injuries - a costly experience for teams, and an unpleasant one for both players and soccer fans.  

A large collection of player level information was used to explore the existing relationship of many variables and injury risk, using visual and statistical modeling methods.  

While it was not possible to establish an accurate prediction system for individual injuries, the analysis demonstrates that there are other valuable insights that could be gathered. Among the most important ones are the impact of past injuries on future risk.  

There are believed to be two roadblocks to high-accuracy predictions: 1. Injuries are rare events, which makes it hard to model them from a technical perspective 2. Any dataset of historical player injuries is inherently biased, as coaches and teams already apply their own mitigation methods.  

The last section contains suggestions for future work, among them an interesting concept of investigating injuries in multiple-day time windows, rather than treating them as in-game occurrences.   

\pagebreak

# Scope

Data used for this exercise is a per game per player level collection of the Premier League matches between 2003 and 2018.  
This data is publicly available, however not in organized format, hence it cannot be shared without explicit permission.  

The target variable for prediction is "injured" - 1 if player was reported injured after the game, 0 otherwise. Descriptive data is also available on the type of the injury, and how long it reportedly last.  

The list of Other variables can be found in the Appendix.

\pagebreak

# Data Preparation

## Filters

As the original dataset was in raw format (as collected from the internet), certain filters were needed to be applied before data was ready for meaningful modeling work.

### Years  

Years before 2010 were removed as their injury data was non-complete.  
Year 2018 was removed as data was incomplete - the Premier League season was still in progress at the time of data collection.

### Missing data  

In certain cases fields contained missing values (NAs) for some records.  
Given the limited number of rows affected, they were removed from the dataset, rather than NAs being imputed by other values.  
Fields affected:  
* Foot  
* Games (or minutes) played  
* Height & Weight  

### Injury types & length  

Unfortunately soccer injuries come in many different formats and their seriousness can be of a wide range.   

The aim in this project is not to predict all possible injury types. The goal is to help decision makers (coaching staff) monitor the players' physical condition, and manage their playing time if neccessary for avoiding typical soccer injuries.  

```{r, out.width = '85%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/3. Injury Types.jpeg")
```

Any successful prediction exercise would need to make sure that the event being predicted does have a statistical connection to the predictors. For many injury types, we know that their occurance is unrelated to outside conditions (at least to our current knowledge). Examples include knocks, concussions, etc. This analysis is not interested in these.  
  
Rather, the injuries we are after are the "wear and tear" type - breakdowns of the human body related to increased physical stress, either in intensity or length (Unfortunately, most of the available data describes length - e.g. games played / rather than intensity - e.g. maximum speed).  
Based on their mechanical relationship and similar length profiles, injuries to be in focus are:  
1. Hamstring  
2. Groin Strain  
3. Calf Muslce Strain  
4. Thigh Muscle Strain  

Even looking at the same injury types sometimes can be "apples to oranges", as their severity might be significantly different. The question "How to measure the severity of an injury?" can be answered many ways.  

In this analysis, the descriptive data available on injuries is their reported length. As seen above, wide ranges are covered. Should we treat a hamstring strain lasting 1 day the same as one lasting 2 months?  

How this question is handled is an important part of the analysis. One could certainly try to weight injuries by their length. This would be a good input into a cost-benefit analysis of resting players for "high-risk" matches.  
Note, however, that to avoid introducing unneccessary complexity, injuries lasting less than 2 weeks will not be considered an injury for the purposes of this analysis.  

This does remove a large part of the "injured" data (see below - removed partition is marked by orange), but it helps create a more homogenous categorization.

```{r, out.width = '50%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/4. Hamstring Injury Lengths.jpeg")
```  
  
### Position  
```{r, out.width = '50%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/6. Injuries vs Position.jpeg")
``` 

Goalkeepers were removed from the population, as their behaviour on the court is radically different from that of other positions - as we see, their injury rates are significantly lower as well. Focus going forward is on field players.  
  
Records with missing position were also removed - upon structural checks, the quality of these rows were not deemed truthworsty for further investigation.  


### Playing time  
```{r, out.width = '50%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/5. Injury rates vs PL season minutes.jpeg")
``` 

No filter (other than removing NA values) were applied to playing time fields.  
While it is a reasonable thought that minimal playing times (either for some bench players, or early season records) could be not contributing to injuries, data suggests otherwise.  
The above chart supports this - records with less than 180 season minutes have no lower injury rates than others.  

## Feature Engineering

There are certain variables that were not available in the raw data format, however they could be calculated / extracted, with the aim of enhancing the analytical separation between injured / non-injured cases.  

Below is a comprehensive list of all variables created during the data preparation:  
* Weekday and Month    
* BMI  
* Team and Opponent  
* Birthplace and Nationality  
* Kick-off time (time of the day)  
* Injury "history" (indicator whether player was injured in a previous time window)  
* Games played in a given time window (career / last year / last 90 days)  
* Some variables were grouped into larger categories (e.g. Venue, Team)  

*An important technical note on categorical variables: before feeding them into predictive models, all categoricals were dummy-encoded.*  

## Exploratory Analysis

### Intro, explain chart types  

In this section, injury rates (number of injuries / number of records) will be broken down by categories of different variables.  

For each variable, there are 2 charts (from left to right):   
1. Error bar encoding the 95% confidence interval of injury rates in a given category. Point at the mean, point size encodes number of games in the category.  
2. Average injury length in a given category, breaken out by injury type  

Commentary will be only provided on the variables deemed most important by the machine learning models applied.

### By physical attributes - age, height, weight, BMI
```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/8g. Injuries vs Age.jpeg")
```

Age has a more or less positive linear connection to the likeness of an injury - the older the player is, the more likely the injury.  
It has to be noted however that the impact is not very large, specially considering that the two "outlier" groups (youngest/oldest) do not contain a large amount of players.  

```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/9g. Injuries vs Weight.jpeg")
```

```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/10g. Injuries vs Height.jpeg")
```

```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/11g. Injuries vs BMI.jpeg")
```

Even though BMI is considered a significant predictor in some models, visual analysis does not uncover any clear trends.  
This might be due to an unlinear relationship - tree-based methods put more weight on BMI than linear ones.  

### By Kick-off time

```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/12g. Injuries vs Kick-off Time.jpeg")
```

### By year, month & weekday
```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/13g. Injuries vs Year.jpeg")
```

There was a clear trend during the years in injury frequencies, first an increase leading up to 2015, and drops in consecutive years since then.  

```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/14g. Injuries vs Month.jpeg")
```

```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/15g. Injuries vs Weekday.jpeg")
```

### By nationality & birthplace
```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/17g. Injuries vs Nationality (Regionalized).jpeg")
```

```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/18g. Injuries vs Birth Country (Regionalized).jpeg")
```

Nationality and birthplace had been selected by multiple models. Most notably, players from Africa are prone to getting injured somewhat more frequently.  

### By team, opponent and venue
```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/19g. Injuries vs Team.jpeg")
```

```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/20g. Injuries vs Opponent.jpeg")
```

```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/16g. Injuries vs Venue.jpeg")
```

Team and venue both have considerable importance according to models. This might be a sign of the impact of playing styles (e.g. speed, physicality). However, these variables are just "distant" proxies for style of play, hence no theories should be made on casuality.  

### By games & minutes played  

Aggregate data shows a reverse relationship between play-load before a match and injury frequencies.  

This is counterintuitive at first - the expectation is that the more playing time, the more load on a players body, leading to higher risk of injury.  

There might be a selection bias however - players who "survive" without being injured will be available to play the most time during a season. This work will only note this discrepancy, and not provide a commentary on causality in lack of information.  

```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/21g. Injuries vs PL Games in Season.jpeg")
```

```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/22g. Injuries vs All Games in Season.jpeg")
```

```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/23g. Injuries vs Non-PL Games in Season.jpeg")
```


### By injury history

<span style="color:red">##TODO: add charts</span>

\pagebreak

# Modeling
## Methodology
### Avoiding overfitting  

To avoid overfitting, a standard approach of separating training / validation / performance sets was used.  

Year 2017 is set aside for final performance evaluation:
```{r, eval = FALSE, echo = TRUE}
training_set    <- original_sample %>%
                     filter(Year < 2017)

performance_set <- original_sample %>%
                     filter(Year == 2017)
```


Data from other years is split into a training and test set, based on a 70% - 30% split:
```{r, eval = FALSE, echo = TRUE}
training_ratio <- 0.70

train_indices <- createDataPartition(y = training_set[["injured"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)

data_train <- training_set[train_indices,  ] %>% as.data.frame()
data_test  <- training_set[-train_indices, ] %>% as.data.frame()
```

*Note: cross-validation was used on the training set, for model tuning*  

## The problem of class imbalance

Based on the previous definition of "injured", the cleaned data set consist of only 1.02% of positive cases.
This makes it highly imbalanced, something that can hurt model performance for classification tasks.  

More precisely: *"doing classification when classes are highly imbalanced leads to underestimation of conditional probabilities of the minority class"* [@imbalanced4].

Specifically looking at the injury prediction problem: the cost of missclassifying a case-of-interest (injured == YES) observation is higher than the cost of a reversed error. It's very important to know when a player is in risk of getting injured, as if the event occurs, he can be out for a long period of time, costing the team money and available resource to compete as well. On the other hand, if a player is in good shape, an extra rest day can only hurt so much.

In machine learning application there are multiple ways of addressing this issue [@imbalanced3]. Main ones include, but are not limited to:  
1. Sub-sampling  
2. Cost sensitive learning - weighting observations  
3. Learning methods, like one-class classifiers (e.g. autoencoders)  

Let's review 1. in more detail, which was used for this analysis.

### Sub-sampling techniques for learning from imbalanced data

There are three possible ways to sub-sample data for better predictive performance: up-sampling, down-sampling, and methods mixing the two, like SMOTE. Each carryies their own benefits and disadvantages, as per [@imbalanced1].

1. Down-sampling the majority class
In this case wer are removing observations from the majority class to make the data more balanced. Usually this method is best when the available data size is large.
It can reduce burden of computation effort and storage, however on the flip side it could potentially remove useful information.

2. Up-sampling the minority class
Oversampling repeats examples of the minority class, usually via bootstrapping.
There is no information loss that happens with down-sampling, however computation times can increase.
Also, by repeatation of the same observations, this method can lead to overfitting.

3. SMOTE: Synthetic Minority Over-sampling TEchnique
An alternative approach is "mixing" the above two techniques. Down-sampling has an obvious drawback in loss of information. Up-sampling also risks to identify more similar, but also more specific regions of the feature space, hence not being able to generalize well in certain cases.
SMOTE [@smote] creates synthetic new examples from the minority class, rather than just repeating existing observations.
The method is actually inspired by an older image recognition technique, where the same pictures are getting slightly distorted (e.g. via rotation).
The idea behind SMOTE works well in theory - however, depending on the data it might introduce new problems. Such a problem could be "noisy" data - when cases of classes are not well separated, SMOTE can enlarge this overlap, as it does not take distribution into consideration when creating synthetic new examples. For similar reasons, SMOTE tends to not work well for high dimensional data.


Another important aspect of sub-sampling is when to apply it in the modeling workflow [@imbalanced2].
If subsampling is done before fitting a model, it will introduce two problems:  
1. For model tuning, the held-out sets in CV will also be sub-sampled. This can create unrealistic or overly optimistic performance measures.  
2. Added uncertainity: we won't know for sure how results would look like with a different subsample. Similar effects arise as in the point above.  


### A practical comparison of different sub-sampling methods

Below is a summary of different sub-sampling techniques applied to the injury prediction problem.  For this analysis, similar Random Forest models were tuned with the different sampling techniques.  

*Note: Technical details of the model fitting process are covered in a later section. For now, just note that the "caret" R library [@caret] was used to create all models. Caret allows for a nice and consistent user interfence, hence it makes it easier to comprehend code referenced in this document. Random Forests were fit with the "ranger" library [@ranger], while SMOTE sampling uses the "DMwR" library [@DMwR].*  

#### AUC and ROC curves  
First, let's refer to the all-in-one metric of classification problems, area under the curve:  

```{r}
readRDS(file = "C:/Users/tkonc/Documents/pl_injuries/Data/data_subsampling_auc.RDS") %>%
  kable(digits = 4,
        align = NULL) %>%
  kable_styling(full_width = F)
```

As seen above, AUC values are not creating a clear separation between different sampling techniques. SMOTE does somewhat better than others, but the differences are not significant.  

This calls for a more detailed inspection, starting with the ROC curves:  

```{r, out.width = '65%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/999. ROC Curve of different sampling strategies.jpeg")
```

ROC curves are also very similar. SMOTE may be somewhat better performing at low and high specificity rates, while others seem better in the middle. Now let's consider something: the real optimization problem is maximizing the time a player can spend on the soccer field. The costliest case is when an injury event is not detected - false negatives can create a lot more headache than false positives. Or so it seems at first - but the possible number of false positives is a lot higher than that of false negatives. This problem will be explored further in the cost-benefit analysis section.  

Neither sampling method is particularly strong, which projects the issues of using models for decision making of player resting.  
This is not he only use-case of analytics however: ranking players by injury risk, or understanding the driving forces behind injury probabilities can also provide value to either coaches, or also to front-offices evaluating different players.  

The first aspect, how realistic the probabilities predicted are on average, will be reviewed here. The impact of different variables are covered when discussing modeling results.

#### Probability calibrations  

Any sub-sampling method will modify the a-prior probabilites of the training data, hence the predicted probabilities of the classifier will be biased. 

The below chart demonstrates this problem in practice:
```{r, out.width = '100%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/999. Uncalibrated probability plots.jpeg")
```

Many methods exist for calibration of sample-biased probabilites. Practical and simple, formula based solutions can be found in [@calibration1] and [@calibration2]. Other methods include regressing the target variable to predictions, but this is not a feasible approach if the original dataset is highly imbalanced.  

This analysis implemented the rescaling as below, where original_fraction is the ratio of the minority class in the original sample, oversampled_fraction is the biased predicted ratio, and score is the individual prediction for a given instance.
```{r, echo = TRUE, eval = FALSE}
f_recalibrate <- function(original_fraction, oversampled_fraction, score) {
  per_p <- 1 + (1 / original_fraction - 1) / (1 / oversampled_fraction - 1) * (1 / score - 1)
  p <- 1 / per_p
  
  return(p)
}
```

Application of this correction results in the new probabilites can be seen below. As demonstrated, these probabilites are better in line with actual occurance ratios.  

```{r, out.width = '100%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/999. Calibrated probability plots.jpeg")
```

Again, neither model does particularly well. SMOTE is strongest however in a key issue: on average, it separates cases with different risk levels much better than any other method. No sampling and up-sampling does not allow for any confident ranking, while down-sampling creates more biased estimates for higher risk instances.  
  
This fact, together with the theoretical advantages, and the computational effort aspect makes SMOTE the best choice for conducting further analysis.

## Methods used

### Overview  

Each method was called with the same caret control function:  
```{r, eval = FALSE, echo = TRUE}
ctrl <- trainControl(method = "cv", 
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary, 
                     sampling = "SMOTE")
```

Models were tuned via 10-fold cross-validation. Other parameters instruct caret that this is a binary classification problem, and that SMOTE-sampling should be used during the fitting process.  

Seeding was applied before each fitting call, to facilitate reproducibility of the results.  
```{r, eval = FALSE, echo = TRUE}
set.seed(93)
```

A total of 5 different methods were fit and tuned to select the best one for predicting injuries.  
Below is a review of them one-by-one to understand the differences.  

### Logit and Probit (GLM)

Logit and Probit are both linear classification methods. They are different in the function which maps the result to the 0 - 1 space (so that it can be used as a probability), which also defines interpretation of coefficients.  

These methods work best when it is important that model details could be understood - as a down-side however, they are not able to capture non-linear effects and complex relationships (without further feature engineering), and the lack of regularization could drive them to overfit if the data is high-dimensional.  

Logit code:  
```{r, eval = FALSE, echo = TRUE}
train(injured ~ .,
      data      = data_train,
      method    = "glm",
      family    = binomial(link = "logit"),
      metric    = "ROC",
      trControl = ctrl)
```
  
  
Probit code:
```{r, eval = FALSE, echo = TRUE}
train(injured ~ .,
      data      = data_train,
      method    = "glm",
      family    = binomial(link = "probit"),
      metric    = "ROC",
      trControl = ctrl)
```

### Reguralized Logit (GLMNet)

GLMNet [@glmnet] uses an elastic-net penalty which balances the use of lasso and ridge regularization.  
The regularization shrinks the coefficient of unimportant variables towards zero, hence it can be used as a variable selector as well. The tuned regularization ensures that the model is not overfit to training data, but GLMNet still does not automatically address the non-linear components.

GLMNet code:
```{r, eval = FALSE, echo = TRUE}
tg_glmnet <- expand.grid(alpha  = c(1:10 / 100),
                         lambda = c(1:10 / 100))

train(injured ~ .,
      data       = data_train,
      method     = "glmnet",
      family     = "binomial",
      metric     = "ROC",
      trControl  = ctrl,
      preProcess = c("center", "scale"),
      tuneGrid   = tg_glmnet)
```

The tuneGrid parameter instructs caret to complete a grid search of provided alpha and lambda parameters.
Standardization is applied, as necessary for regularization.

### Random Forest (Ranger)

Random Forest is a tree-based method which requires minimal tuning in exchange for strong performance characteristics.
The trees ensure that non-linear patterns are also taken into account for prediction. Random Forest ensambles many decision trees, each built on a bootstrapped sample of the training data. At each step, only a random selection of features is used for decision on best split, hence it reduces the likelihood of relying too much on a small set of variables. This makes random forests very reluctant to overfit.  

Random Forest code:  
```{r, eval = FALSE, echo = TRUE}
tg_rf <- expand.grid(.mtry          = c(2:12),
                     .splitrule     = "gini",
                     .min.node.size = c(1:8 * 25))

train(injured ~ .,
      data       = data_train,
      method     = "ranger",
      metric     = "ROC",
      trControl  = ctrl,
      tuneGrid   = tg_rf,
      num.trees  = 1000,
      importance = "impurity")
```

A 1000 trees were fit to make predictions. Tuning parameters included the number of random features selected at each split, the splitting rule, as well as the minimum node size in each tree.  

### Gradient Boosting (XGBoost)

XGBoost [@xgboost] is a widely-used implementation of extreme gradient boosting. The workings and parameters of XGBoost is well covered [here](https://github.com/dmlc/xgboost/blob/master/R-package/vignettes/xgboostPresentation.Rmd).  

While gradient boosting tends to perform better than random forests on many problems, they are complicated to fit well, due to the large number of tuning parameters. Training XGBoost, while reasonabily fast, is also significantly slower than training Random Forests (with *ranger*), due to the same reason.  

XGBoost code:
```{r, eval = FALSE, echo = TRUE}
tg_xgb <- expand.grid(nrounds          = c(250, 500),
                      max_depth        = c(5, 10, 15),
                      eta              = c(0.1, 0.25),
                      gamma            = c(0.1, 0.25),
                      colsample_bytree = c(2:6 / 10),
                      min_child_weight = c(1:4),
                      subsample        = c(5:9 / 10))

train(injured ~ .,
      method    = "xgbTree",
      metric    = "ROC",
      data      = data_train,
      trControl = ctrl,
      tuneGrid  = tg_xgb)
```


## Results

### Predictions

As with before, evaluation will start with reviewing AUC values and ROC curves for models being compared.  

#### Evaluation based on AUC and ROC curves
```{r}
readRDS(file = "C:/Users/tkonc/Documents/pl_injuries/Data/data_eval_auc.RDS") %>%
  kable(digits = 4,
        align = NULL) %>%
  kable_styling(full_width = F)
```

Random Forest has a significant, 0.025 lead over the second best model, XGBoost.

```{r, out.width = '85%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/9999. ROC Curves for different models.jpeg")
```

This is due its ability to better classify positives under lower cut-off thresholds, as demonstrated by the ROC curve. XGBoost perform similarly in the upper-range of the FPR-scale.  

The linear models are somewhat lagging behind, specially without regularization. This is a good sign that injury probability not always has a linear relationship with the predictor variables.  

Unfortunately, no model has a very strong performance. This indicates that classifications could not be a primary decision-making input. Consequences, and mitigation will be covered in sections "Cost-benefit analysis" and "A time window concept for injuries", respectively.  

#### Probability calibrations  

Even if predicted probabilities are not accurate enough on their own, they do have value, if their ability to rank players from low to high risk is good, at least on the average.  

```{r, out.width = '85%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/9999. Calibrated probability plots.jpeg")
```

The two models that produce relatively well-scaled predictions are GLMNet and Random Forest. Suprisingly, XGBoost does not handle well the imbalance - it correctly assumes that most cases have fallen into the very-low probability group, however it is not able to predict on scale for higher risk players.  

Based on the above, Random Forest is selected as the best possible model for prediction of injuries. An evaluation of practical value in player resting is covered in the next section.  

### Cost-benefit analysis
To evaluate the "business" value of the proposed model consider the following evaluation framework:  
1. Benchmark is actual missed time - sum of injury length
2. Model-based missed time is calculated as follows: if a player is classified as injury-risk, he will not play in the match. A 3.5 days missed length is assumed. If a player is not classified as injury-risk, but gets injured, the actual injury length will be counted  
  
Benchmark data is from year 2017, which was not used for model tuning and evaluation previously.

The below chart demonstrates the impact of chosing different thresholds:

```{r, out.width = '65%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/xxx. Cost-benefit.jpeg")
```

Unfortunately, it seems that even in the best case, the benefits are neglible. And if a slightly different threshold is chosen, the modeled approach is worst-off than the historical benchmark.  

To understand why, consider the confusion matrix visualized at a reasonable threshold rate:
```{r, results = 'asis'}
readRDS(file = "C:/Users/tkonc/Documents/pl_injuries/Data/data_cm.RDS") %>% ftable %>% pander()
```

The true positive rate, the rate of identified injuries is 14.85%. On the other hand, false positive classifications are happening at a below 4% rate. At first this seems like a good alternative to the no-model case.  

The problem is that false positives are not costless. Positive classifications mean that the player needs to miss the upcoming match (in the analysis measured as 3.5 days missed). The average missed time for an injury is approximately 37 days. With these costs, the model would need to identify around one tenth as many true positives as false positives to be on par with the benchmark.  

This is a case of "no free lunch" in data science - mistakes are costly, hence just because something can be modeled, it does not mean that it can be modeled benefically for the business. The likely challenge (apart from a possible large amount of randomness to injuries) is that teams already try to optimize against injuries - public data might not be enough to beat the best soccer managers in the World.  

### Variable importance  

Caret allows for an easy extraction of relative variable importances for any model kind. This makes it possible to compare the relative importances across models, even if their mechanics are different.
```{r, out.width = '85%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/9999. Variable Importance across models.jpeg")
```
The first thing that can be said is that different models weight variables very differently. This is likely due to differences in their ability to grasp non-linearities, as well as being caused by the high correlation among variables.  

Focusing on the similarities across all model types:  
1. Importance of past injuries  
2. Importance of age  
3. The impact of amount played recently, measured by many variables
4. And the differences between positions  

These results are aligned with the results of visual expoloration.  

Random Forest and XGBoost does not allow for an easy interpretation of the direction of the relationship between injuries and a variable, however GLMNet coefficients can be considered for some insight:  

```{r, out.width = '85%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/9999. Coefficent values for GLMNet models.jpeg")
```
<span style="color:red">##TODO</span>

### Considerations regarding external validity

Even if model predictions can not be considered accurate enough to be direct inputs to decision making on player resting, the relationships uncovered between different factors and injury risk is an important insight on its own.  

It needs to be considered however if the results can be seen as good generalizations. There are two important aspects that should be weighted: whether the results contain any bias not modeled in the data, and whether the results would project well into space not covered by the data.  

#### Model Bias
Model bias arise when predictions are skewed by certain factors not transparently to model owners.  
Usually, certain dormant variables (not measured) will have a casual relationship with the target being analyzed. As the real causes are not measured, proxies are being applied, which tend to show high correlation with the underlying.  

If the desired variables are not available, caution is needed to ensure that the model is not polluted by unintended (and untrue) consequences.  

For the injury prediction problem, the performance across years is to be considered.  
It is understood that injury rates across years are not constant, but year itself cannot be a predictor, which creates a modeling challenge.

```{r, out.width = '45%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/xxx. Prediction bias over years.jpeg")
```

In some sense unfortunately the models do fail this test - their predictions are relatively flat over years.
It cannot be concluded however that this is a problem with the models - occurances of injuries have a random factor for sure, and it was shown that the 95% confidence intervals of actual injury rates are rather wide. What we know is that modeled variables did not show a significant variation over years, but injury frequencies did. This might be just due to random chance.  

#### Data-external factors
It should be remembered that data is for the Premier League, seasons between 2010 and 2017.  
Each major soccer league has its own playing styles and player compositions at any given time. Some play at a more uptempo pace, with more physicality. In others the focus is rather on finesse. Players can be very different as well - while the Premier League, being among the richest and most historious leagues, have players from all over the world, it is still majorly an English league. This is true for its players and personel as well.  

Another aspect is weather. British weather is famous for not being friendly, but compared to more sunny, humid or warm locations the injury risk might be different as well.  

This factors are just a question of data, and can be controlled for.  

A harder aspect to measure is each player's individual physical (and psychical) condition. While their is more and more possibility to gather data on these aspects, it is unlikely that it will be made accessible to the public any time soon.


\pagebreak

# Considerations for future work
A clear challenge in injury prediction with a public dataset is that there is no clear indication whether injuries are truly random to the extent of failing to predict them accurately, or if the role of coaches (and their intervention to prevent injuries of players who they see in risk) inherently biases the data. Analysis showed no clear indications that certain there would be significant differences among teams.  

As it is generally the case, better data could help enhance the analysis. Specifically more variables indicating player conditions, as well as more descriptive data on past play (distances run, sprints, etc.) could be used to better separate the workload of players before each game.  

## A time window concept for injuries
An interesting idea was present in a recent conference paper [@nba] on in-game injuries happening in the NBA.  

The authors present a time window technique: rather than trying to predict if a player will get injured during a game, they use a multiple-day prediction window for injuries.  

Theoretically, the same idea can be applied to soccer injuries. While this project does not provide a deep investigation into the workability of the concept, a brief look is provided into what it could offer.  

```{r, out.width = '85%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/xxx. Injury rates by Time Windows.jpeg")
```

In total 5 Random Forest models were trained (similarly as before). The comparison of their ROC curves can be seen below:  
```{r, out.width = '85%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/99999. ROC curves for different prediction time windows.jpeg")
```

From the prediction perspective, it looks like that an ideal time window is somewhere in the 14 - 21 days range. It should be noted however, that even with an improvement in the TPR / FPR trade-off, a cost-efficient method could not be implemented, as the ratio of false positives is still too high.  

Different time windows can be calibrated similarly - while none is perfect, each one is reasonable in terms of risk ranking:  
```{r, out.width = '85%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/99999. Calibrated probability plots for different time windows.jpeg")
```

Last but not least, all windows value the importance of different factors (variables) similarly, even though not identically. This confirms the role these factors have in injury risk, but the differences also encourge for further feature insepction.
```{r, out.width = '75%', fig.align = 'center'}
include_graphics("C:/Users/tkonc/Documents/pl_injuries/Figures/99999. Variable Importance across prediction time windows.jpeg")
```

\pagebreak

# Appendix 
## Contact details
The author of this paper can be contacted directly via *t.koncz@gmail.com*.  

## Project organization

This project is breaken into separte, runable parts, which are not included in this markdown file.
The main parts are:  
1. Raw Data  
2. Runable R scripts, storing the steps of the analysis  
3. Functions  
4. Models  
5. Notebooks  
6. Figures  
7. Data  

Further publicly disclosable details, and specific code is available on the [github repository of this project](https://github.com/tomiaJO/pl_injuries).

## List of variables in raw data

```{r}
kable(data.frame(
"Variable" = c("Date",
"balance",
"ft_a",
"ft_h",
"Attendance",
"Game week",
"Kick-off",
"Venue",
"home",
"ht_a",
"ht_h",
"injured",
"mid",
"minutes",
"pid",
"starter",
"injury_length",
"injury_type",
"injury_minutes_played",
"days_till_injury",
"pl/all_minutes/games_n/season",
"Country of birth",
"Date of birth",
"First name",
"Foot",
"Height",
"Last name",
"Nationality",
"Place of birth",
"Position",
"Weight",
"away_team",
"away_tid",
"home_team",
"home_tid"),
"Description" = c("date of the game (YYYY-MM-DD)",
"absolute full time goal difference=|ft_a - ft_h|",
"full time away team goals",
"full time home team goals",
"number of fans at the game",
"game week of the season",
"kickoff time (GMT)",
"Location of the stadium",
"whether player lines up or the home side or not",
"half time away team goals",
"half time home team goals",
"whether player was reported injured after the match, (before playing any other match) or not",
"unique match id",
"minutes played by the player on the match (stoppage time is discarded, so cannot be more than 90)",
"unique player id",
"whether player started the match or not",
"if player was reported after the game, how many days did the injury reportedly last",
"what type of injury was reported",
"equals the minutes variable in case of injured = 1, otherwise 0",
"how many days between the date of the match and start of reported injury",
"how many games/minutes of premier league/any other football played by player in the n/days before the match (or in the season, total)",
"player country of birth",
"player DoB",
"player first name",
"player preferred foot",
"player height",
"player last name",
"player nationality",
"player place of birth",
"player position",
"player weight (at time of collecting data)",
"away team name",
"away team id",
"home team name",
"home team id")
))

```


## Sessioninfo

```{r}
sessionInfo()
```

## References  
